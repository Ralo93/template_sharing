import pandas as pd
import os
import numpy as np


import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import seaborn as sns


data = pd.read_csv(r'../data/interim/train.csv')


data.info()


data.describe()


data.head()


# Select categorical columns
#categorical_df = data.select_dtypes(include=['object'])
#categorical_df = categorical_df.drop('id', axis=1)
# Select numerical columns

numerical_df = data.select_dtypes(exclude=['object'])
print(len(numerical_df.columns))


data.columns.isna().sum()


data.CH2O.value_counts()


import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.express as px
import plotly as plt
import pandas as pd

# Function to create subplots for numerical and categorical features
def create_feature_subplots(df):
    # Separate numerical and categorical features
    numerical_df = df.select_dtypes(include=['number'])
    categorical_df = df.select_dtypes(include=['object', 'category'])
    
    # Calculate the total number of features
    total_features = len(numerical_df.columns) + len(categorical_df.columns)
    
    # Calculate the number of rows needed for the subplots (3 columns layout)
    num_rows = int(np.ceil(total_features / 3))
    
    # Create subplots
    fig = make_subplots(
        rows=num_rows, 
        cols=3, 
        subplot_titles=[f"Distribution of {col}" for col in numerical_df.columns] + [f"Distribution of {col}" for col in categorical_df.columns],
        shared_xaxes=False
    )
    
    # Color palette for plots
    colors = px.colors.qualitative.Plotly

    # Plot numerical features as histograms
    for i, col in enumerate(numerical_df.columns):
        # Calculate the correct row and column position
        row = (i // 3) + 1
        col_position = (i % 3) + 1
        
        # Add histogram trace without binning
        fig.add_trace(
            go.Histogram(
                x=numerical_df[col],
                name=col,
                marker=dict(color=colors[i % len(colors)], line=dict(width=1, color='black')),
                opacity=0.75,
                showlegend=True
            ),
            row=row,
            col=col_position
        )
        
    # Plot categorical features as bar plots
    start_index = len(numerical_df.columns)
    for j, col in enumerate(categorical_df.columns):
        # Calculate the correct row and column position
        row = ((start_index + j) // 3) + 1
        col_position = ((start_index + j) % 3) + 1
        
        # Calculate value counts for the categorical feature
        value_counts = categorical_df[col].value_counts()
        
        # Add bar trace
        fig.add_trace(
            go.Bar(
                x=value_counts.index,
                y=value_counts.values,
                name=col,
                marker=dict(color=colors[(start_index + j) % len(colors)], line=dict(width=1, color='black')),
                opacity=0.75,
                showlegend=True
            ),
            row=row,
            col=col_position
        )
    
    # Update layout for a better look
    fig.update_layout(
        height=300 * num_rows,
        width=1500,
        title_text="Distributions of Features",
        title_x=0.5,  # Center the title
        template='plotly_white'  # Use a clean template
    )
    
    # Update x-axis and y-axis titles
    fig.update_xaxes(title_text="Value", showgrid=True)
    fig.update_yaxes(title_text="Count", showgrid=True)

    fig.show()
    # Show the figure
    return fig
    

# Example usage:
# Assuming 'data' is your DataFrame with numerical and categorical features
fig = create_feature_subplots(data)



import matplotlib.pyplot as plt


corr_matrix = numerical_df.corr()

plt.figure(figsize=(12, 10))

# Create a heatmap
sns.heatmap(corr_matrix, 
            annot=True,         # Show correlation coefficients
            fmt=".2f",          # Format the annotations to two decimal places
            cmap='coolwarm',    # Color map
            linewidths=0.5,     # Lines between squares
            square=True,        # Make squares
            cbar_kws={"shrink": .5})  # Color bar size


from sklearn.feature_selection import mutual_info_classif



data.head()


from sklearn.feature_selection import SelectFromModel
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import mutual_info_classif
from sklearn.preprocessing import LabelEncoder

x = data.loc[:, ['Gender', 'Age', 'Height', 'Weight', 'family_history_with_overweight', 'FAVC', 'FCVC', 'CAEC', 'SMOKE', 'CH2O', 'SCC', 'FAF', 'TUE', 'CALC', 'MTRANS']]
y = data.loc[:, 'NObeyesdad']

# Calculate mutual information for classification with automatic encoding for categorical features
def compute_mi_classification(X, y):
    X_encoded = X.copy()
    for col in X_encoded.select_dtypes(include=['object', 'category']).columns:
        X_encoded[col] = LabelEncoder().fit_transform(X_encoded[col].astype(str))
    mi_scores = mutual_info_classif(X_encoded, y)
    # Create a dictionary with column names and their MI scores
    mi_dict = dict(zip(X.columns, mi_scores))
    mi_dict_sorted = dict(sorted(mi_dict.items(), key=lambda item: item[1], reverse=True))
    return mi_dict_sorted

# Example usage:
mi_scores = compute_mi_classification(x, y)

# Create a bar chart
fig = go.Figure(data=[
    go.Bar(
        x=list(mi_scores.keys()),
        y=list(mi_scores.values()),
        marker=dict(color='skyblue'),
        text=[f"{v:.3f}" for v in mi_scores.values()],
        textposition='auto'
    )
])

# Update layout
fig.update_layout(
    title="Mutual Information Scores with target",
    xaxis_title="Features",
    yaxis_title="Mutual Information Score",
    template='plotly_white',
    height=500
)

# Show the figure
fig.show()


k = 6
x = x.copy()
for col in x.select_dtypes(include=['object', 'category']).columns:
    x[col] = LabelEncoder().fit_transform(x[col].astype(str))
        
selector = SelectKBest(mutual_info_classif, k=k)
features = selector.fit_transform(x, y)

features





x.columns[sel.get_support()]


from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import ExtraTreesClassifier

test = pd.read_csv(r'../data/raw/test.csv')

x_test = test

# Initialize and train the ExtraTreesClassifier
mdl = ExtraTreesClassifier(n_estimators=50)
mdl.fit(x, y)

# Select features using the trained model and a mean threshold
model = SelectFromModel(mdl, prefit=True, threshold='mean')
x_new = model.transform(x)  # Transformed training data

# Display selected features
selected_features = x.columns[model.get_support()]
print(f"Selected Features: {selected_features}")

# Transform the test set with the same selected features
x_test_new = model.transform(x_test)

# Make predictions on the test set
y_pred = mdl.predict(x_test_new)

# Calculate and print classification metrics
print("Classification Report:")
print(classification_report(y_test, y_pred))

# Print accuracy score
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")



