import pandas as pd
import os
import numpy as np
import sys
sys.path.append('..')


os.chdir(r'C:\Users\Administrator\Desktop\raphi_other\repositories\template_project\src')


data = pd.read_csv(r'../data/interim/house_data.csv')


#test = pd.read_csv(r'../data/interim/test.csv')


numerical_df = data.select_dtypes(exclude=['object'])
categorical_df = data.select_dtypes(include=['object'])


columns_to_drop = ['state']#, 'SCC', 'SMOKE', 'FAVC', 'MTRANS']
data = data.drop(columns=columns_to_drop)


x = data.drop(columns=['price'])
y = data.price


y


x.to_csv('../data/interim/house_xinit.csv', index=False)
y.to_csv('../data/interim/house_yinit.csv', index=False)


#y.value_counts()





y = pd.Series(y, name='y')


y


numerical_df = x.select_dtypes(exclude=['object'])
categorical_df = x.select_dtypes(include=['object'])


numerical_df


categorical_df


# Select categorical columns with relatively low cardinality (convenient but arbitrary)
categorical_cols = [cname for cname in x.columns if x[cname].nunique() < 8 and x[cname].dtype == "object"]

# Select numerical columns
numerical_cols = [cname for cname in x.columns if x[cname].dtype in ['int64', 'float64']]


categorical_cols


from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder,OrdinalEncoder,StandardScaler

#train_test_split
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)


# X_train.to_csv('../data/interim/x_train_init.csv', index=False)
# X_test.to_csv('../data/interim/x_test_init.csv', index=False)
# y_train.to_csv('../data/interim/y_train_init.csv', index=False)
# y_test.to_csv('../data/interim/y_test_init.csv', index=False)


X_train.head()


print(y_train)


from category_encoders import TargetEncoder


#create numerical transformer
numerical_transformer = Pipeline([('imputer', SimpleImputer(strategy='mean')), 
                                  ('scaler', StandardScaler()) ])

#create categorical transformer
categorical_transformer = Pipeline(steps=[ ('imputer', SimpleImputer(strategy='most_frequent')),
                                            ('onehot', OneHotEncoder(handle_unknown='ignore'))
                                            ])

target_encoding_pipeline = Pipeline(steps=[
    ('target_encoder', TargetEncoder(cols=['town']))
])


# Combine the transformations using ColumnTransformer
preprocessor = ColumnTransformer(transformers=[
    ('target_town', target_encoding_pipeline, ['town']),  # TargetEncoder for 'town'
    ('num', numerical_transformer, numerical_cols),       # Transformer for numerical columns
    ('cat_onehot', categorical_transformer, categorical_cols) # OneHotEncoder for other categorical features
], remainder='passthrough')


# #column transformer
# preprocessor = ColumnTransformer(
#                                 transformers=[
#                                     ('num', numerical_transformer, numerical_cols),
#                                     ('cat', categorical_transformer, categorical_cols)
#     ])





#show decision tree
import matplotlib.pyplot as plt

plt.rcParams["figure.figsize"] = (80,40)
tree.plot_tree(rf,filled = True);
#plt.savefig('./images/tree_example.png')


from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.pipeline import Pipeline
from sklearn import tree

rf = RandomForestRegressor(random_state=42, criterion='squared_error')

# Model pipeline
rf_pipe = Pipeline(steps=[('preprocessor', preprocessor),
                          ('random_forest', rf)
                         ])

# Fit model on training data
rf_pipe.fit(X_train, y_train)

# Get predictions on test data
rf_preds = rf_pipe.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, rf_preds)
mae = mean_absolute_error(y_test, rf_preds)
r2 = r2_score(y_test, rf_preds)

# Print evaluation metrics
print(f'Mean Squared Error (MSE) for a single tree Model: {mse:.2f}')
print(f'Mean Absolute Error (MAE) for a single tree Model: {mae:.2f}')
print(f'R-squared (R2) for for a single tree Model: {r2:.2f}')



#show decision tree
import matplotlib.pyplot as plt

plt.rcParams["figure.figsize"] = (80,40)
tree.plot_tree(rf,filled = True);
#plt.savefig('./images/tree_example.png')


import plotly.graph_objects as go
# Get feature names from the preprocessor
feature_names = preprocessor.get_feature_names_out()

importances = rf.feature_importances_

# Get the feature importances from the Random Forest model
# Create a dictionary mapping feature names to their importances
feature_importance_dict = dict(zip(feature_names, importances))

# Optional: Sort the dictionary by importance in descending order
feature_importance_sorted = dict(sorted(feature_importance_dict.items(), key=lambda item: item[1], reverse=True))

fig = go.Figure(data=[
        go.Bar(
            x=list(feature_importance_sorted.keys()),
            y=list(feature_importance_sorted.values()),
            marker=dict(color='skyblue'),
            text=[f"{v:.3f}" for v in feature_importance_sorted.values()],
            textposition='auto'
        )
    ])

# Update layout for better aesthetics
fig.update_layout(
    title='Feature Importances from Random Forest',
    xaxis_title='Features',
    yaxis_title="Importance Score",
    template='plotly_white',
    height=600
)

# Display the figure
fig.show()



