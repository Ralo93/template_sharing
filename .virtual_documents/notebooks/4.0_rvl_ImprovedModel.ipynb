import pandas as pd
import os
import numpy as np
from catboost import CatBoostClassifier
import logging

# Set up basic configuration
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')



#X_train.to_csv('../data/interim/x_train_init.csv', index=False)
#X_test.to_csv('../data/interim/x_test_init.csv', index=False)
#y_train.to_csv('../data/interim/y_train_init.csv', index=False)
#y_test.to_csv('../data/interim/y_test_init.csv', index=False)


X_train = pd.read_csv(r'../data/interim/x_train_init.csv')
X_test = pd.read_csv(r'../data/interim/x_test_init.csv')
y_train = pd.read_csv(r'../data/interim/y_train_init.csv')
y_test = pd.read_csv(r'../data/interim/y_test_init.csv')


# Select categorical columns with relatively low cardinality (convenient but arbitrary)
categorical_cols = [cname for cname in X_train.columns if X_train[cname].nunique() < 8 and X_train[cname].dtype == "object"]

# Select numerical columns
numerical_cols = [cname for cname in X_train.columns if X_train[cname].dtype in ['int64', 'float64']]


from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder,OrdinalEncoder,StandardScaler
from catboost import CatBoostClassifier
from xgboost import XGBClassifier


#create numerical transformer
numerical_transformer = Pipeline([('imputer', SimpleImputer(strategy='mean')), 
                                  ('scaler', StandardScaler()) ])

#create categorical transformer
categorical_transformer = Pipeline(steps=[ ('imputer', SimpleImputer(strategy='most_frequent')),
                                            ('onehot', OneHotEncoder(handle_unknown='ignore'))
                                            ])


#column transformer
preprocessor = ColumnTransformer(
                                transformers=[
                                    ('num', numerical_transformer, numerical_cols),
                                    ('cat', categorical_transformer, categorical_cols)
    ])





from sklearn.utils.class_weight import compute_class_weight
import numpy as np

# Assuming y_encoded contains integer-encoded class labels starting from 0
classes = np.unique(y_train)
y_train = y_train.iloc[:, 0]
print("Classes:", classes)
print(type(y_train))

# Compute class weights
class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)
print("Class Weights:", class_weights)

# Convert to a dictionary (optional)
class_weights_dict = dict(zip(classes, class_weights))
print("Class Weights Dictionary:", class_weights_dict)





from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report



# Create a CatBoostClassifier model
cb = CatBoostClassifier(
    iterations=300,        # Equivalent to n_estimators
    learning_rate=0.1,
    depth=5,
    random_state=42,
    class_weights=class_weights,
    verbose=0               # Suppress training output
)

# Model pipeline
rf_pipe = Pipeline(steps=[('preprocessor', preprocessor),
                          ('random_forest', cb)
                         ])

# Preprocessing of training data, fit model 
rf_pipe.fit(X_train, y_train)

# Preprocessing of validation data, get predictions
rf_preds = rf_pipe.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, rf_preds)
print('Accuracy for Catboost:', accuracy)

# Detailed classification report
print('Classification Report:\n', classification_report(y_test, rf_preds))



import plotly.graph_objects as go
# Get feature names from the preprocessor
feature_names = preprocessor.get_feature_names_out()


importances = cb.feature_importances_

# Get the feature importances from the Random Forest model
# Create a dictionary mapping feature names to their importances
feature_importance_dict = dict(zip(feature_names, importances))

# Optional: Sort the dictionary by importance in descending order
feature_importance_sorted = dict(sorted(feature_importance_dict.items(), key=lambda item: item[1], reverse=True))

fig = go.Figure(data=[
        go.Bar(
            x=list(feature_importance_sorted.keys()),
            y=list(feature_importance_sorted.values()),
            marker=dict(color='skyblue'),
            text=[f"{v:.3f}" for v in feature_importance_sorted.values()],
            textposition='auto'
        )
    ])

# Update layout for better aesthetics
fig.update_layout(
    title='Feature Importances from Random Forest',
    xaxis_title='Features',
    yaxis_title="Importance Score",
    template='plotly_white',
    height=600
)

# Display the figure
fig.show()


# XGBoost
xgb = XGBClassifier(
    n_estimators=300,
    learning_rate=0.1,
    max_depth=5,
    random_state=42,
    eval_metric='mlogloss'
)

logging.info("Starting Pipeline...")
# Model pipeline
rf_pipe = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('xgboost', xgb)
])

logging.info("Starting fit..")
# Preprocessing of training data, fit model 
rf_pipe.fit(X_train, y_train)

logging.info("Starting test..")
# Preprocessing of validation data, get predictions
rf_preds = rf_pipe.predict(X_test)
logging.info("Done testing")

# Evaluate the model
accuracy = accuracy_score(y_test, rf_preds)
print('Accuracy for XGBoost:', accuracy)

# Detailed classification report
print('Classification Report:\n', classification_report(y_test, rf_preds))


import plotly.graph_objects as go
# Get feature names from the preprocessor
feature_names = preprocessor.get_feature_names_out()


importances = xgb.feature_importances_

# Get the feature importances from the Random Forest model
# Create a dictionary mapping feature names to their importances
feature_importance_dict = dict(zip(feature_names, importances))

# Optional: Sort the dictionary by importance in descending order
feature_importance_sorted = dict(sorted(feature_importance_dict.items(), key=lambda item: item[1], reverse=True))

fig = go.Figure(data=[
        go.Bar(
            x=list(feature_importance_sorted.keys()),
            y=list(feature_importance_sorted.values()),
            marker=dict(color='skyblue'),
            text=[f"{v:.3f}" for v in feature_importance_sorted.values()],
            textposition='auto'
        )
    ])

# Update layout for better aesthetics
fig.update_layout(
    title='Feature Importances from Random Forest',
    xaxis_title='Features',
    yaxis_title="Importance Score",
    template='plotly_white',
    height=600
)

# Display the figure
fig.show()
