import pandas as pd
import os
import numpy as np
import sys
sys.path.append('..')


os.chdir(r'C:\Users\Administrator\Desktop\raphi_other\repositories\template_project\src')


data = pd.read_csv(r'../data/interim/cars_data.csv')


#test = pd.read_csv(r'../data/interim/test.csv')





x = data.drop(columns=['product_tier'])
y = data.product_tier


# x['ctr'] = x['ctr'].astype('float64')


numerical_df = data.select_dtypes(exclude=['object'])
categorical_df = data.select_dtypes(include=['object'])


y


#x.to_csv('../data/interim/house_xinit.csv', index=False)
#y.to_csv('../data/interim/house_yinit.csv', index=False)


#y.value_counts()





order = ['Basic', 'Premium', 'Plus']#, 'Overweight_Level_II', 'Obesity_Type_I', 'Obesity_Type_II', 'Obesity_Type_III']

# Convert 'y' to a categorical type with the specified order
y = pd.Categorical(y, categories=order, ordered=True).codes


y = pd.Series(y, name='y')


y


numerical_df = x.select_dtypes(exclude=['object'])
categorical_df = x.select_dtypes(include=['object'])


numerical_df


categorical_df


# Select categorical columns with relatively low cardinality (convenient but arbitrary)
categorical_cols = [cname for cname in x.columns if x[cname].nunique() < 800 and x[cname].dtype == "object"]

# Select numerical columns
numerical_cols = [cname for cname in x.columns if x[cname].dtype in ['int64', 'float64']]


categorical_cols


from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder,OrdinalEncoder,StandardScaler, MinMaxScaler

#train_test_split
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)


# X_train.to_csv('../data/interim/x_train_init.csv', index=False)
# X_test.to_csv('../data/interim/x_test_init.csv', index=False)
# y_train.to_csv('../data/interim/y_train_init.csv', index=False)
# y_test.to_csv('../data/interim/y_test_init.csv', index=False)


X_train.head()


print(y_train)


#y_train = np.log(y_train)


#y_test = np.log(y_test)


print(y_train)


from category_encoders import BaseNEncoder


from sklearn.preprocessing import FunctionTransformer


# Define the log transformation function
#log_transformer = FunctionTransformer(np.log, 
#                                      inverse_func=np.exp, 
#                                      validate=True)

shifted_log_transformer = Pipeline(steps=[
    ('shift', FunctionTransformer(lambda x: x + 1)),  # Add 1 to avoid log(0)
    ('log', FunctionTransformer(np.log))
])

# Columns to apply log transformation
log_columns = ['price', 'search_views', 'detail_views', 'live_days']


#create numerical transformer



numerical_transformer = Pipeline([('imputer', SimpleImputer(strategy='mean')), 
                                  ('scaler', StandardScaler()) ])

#create categorical transformer
categorical_transformer = Pipeline(steps=[ ('imputer', SimpleImputer(strategy='most_frequent')),
                                            ('onehot', OneHotEncoder(handle_unknown='ignore'))
                                            ])

target_encoding_pipeline = Pipeline(steps=[
    ('base_encoder', BaseNEncoder(cols=['make_name'], base=3))
])


# Combine the transformations using ColumnTransformer
preprocessor = ColumnTransformer(transformers=[
    ('log', shifted_log_transformer, log_columns),
    ('base_name', target_encoding_pipeline, ['make_name']),  # TargetEncoder for 'town'
    ('num', numerical_transformer, numerical_cols),       # Transformer for numerical columns
    ('cat_onehot', categorical_transformer, categorical_cols) # OneHotEncoder for other categorical features
])


# #column transformer
# preprocessor = ColumnTransformer(
#                                 transformers=[
#                                     ('num', numerical_transformer, numerical_cols),
#                                     ('cat', categorical_transformer, categorical_cols)
#     ])





# from sklearn.utils import resample

# sampling_strategy = {
#     0: y_train.value_counts()[0],  # Keep class 0 as is
#     1: y_train.value_counts()[0],  # Oversample class 1 to the same as class 0
#     2: y_train.value_counts()[0]   # Oversample class 2 to the same as class 0
# }

# # Example of undersampling
# oversampler = RandomOverSampler(random_state=42, sampling_strategy=sampling_strategy)
# X_train, y_train = oversampler.fit_resample(X_train, y_train)


from sklearn.utils import resample

# Combine features and target into a single DataFrame
df_train = pd.concat([X_train, y_train], axis=1)

# Rename the target column for clarity (if not already named)
# Replace 'target' with your actual target column name if different
df_train.columns = list(X_train.columns) + ['product_tier']

# Identify the majority class (assumed to be class 0)
majority_class = df_train[df_train['product_tier'] == 0]

# Identify minority classes
minority_class1 = df_train[df_train['product_tier'] == 1]
minority_class2 = df_train[df_train['product_tier'] == 2]

n_majority = len(majority_class)
print(f"Number of samples in majority class (Class 0): {n_majority}")

# Oversample Class 1
minority_class1_upsampled = resample(
    minority_class1,
    replace=True,           # Sample with replacement
    n_samples=n_majority,  # Match number of majority class
    random_state=42         # For reproducibility
)

# Oversample Class 2
minority_class2_upsampled = resample(
    minority_class2,
    replace=True,
    n_samples=n_majority,
    random_state=42
)

# Combine majority class with oversampled minority classes
df_upsampled = pd.concat([majority_class, minority_class1_upsampled, minority_class2_upsampled])

# Shuffle the dataset to ensure random distribution
df_upsampled = df_upsampled.sample(frac=1, random_state=42).reset_index(drop=True)

# Separate features and target
X_train_upsampled = df_upsampled.drop('product_tier', axis=1)
y_train_upsampled = df_upsampled['product_tier']

print("Class distribution before oversampling:")
print(y_train.value_counts())

print("\nClass distribution after oversampling:")
print(y_train_upsampled.value_counts())

X_train = X_train_upsampled #= df_upsampled.drop('product_tier', axis=1)
y_train = y_train_upsampled #= df_upsampled['product_tier']


X_train.make_name.value_counts()


# from imblearn.under_sampling import RandomUnderSampler

# # Example of undersampling
# undersampler = RandomUnderSampler(random_state=42, sampling_strategy='majority')
# X_train, y_train = undersampler.fit_resample(X_train, y_train)


from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Create a RandomForestClassifier model
rf = RandomForestClassifier(n_estimators=150, random_state=42, max_depth=4)

# Model pipeline
rf_pipe = Pipeline(steps=[('preprocessor', preprocessor),
                          ('random_forest', rf)
                         ])

# Preprocessing of training data, fit model 
rf_pipe.fit(X_train, y_train)

# Preprocessing of validation data, get predictions
rf_preds = rf_pipe.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, rf_preds)
print('Accuracy for Random Forest Model:', accuracy)

# Detailed classification report
print('Classification Report:\n', classification_report(y_test, rf_preds))



rf_preds = rf_pipe.predict(X_train)

# Evaluate the model
accuracy = accuracy_score(y_train, rf_preds)
print('Accuracy for Random Forest Model:', accuracy)

# Detailed classification report
print('Classification Report:\n', classification_report(y_train, rf_preds))



import plotly.graph_objects as go
# Get feature names from the preprocessor
feature_names = preprocessor.get_feature_names_out()

importances = rf.feature_importances_

# Get the feature importances from the Random Forest model
# Create a dictionary mapping feature names to their importances
feature_importance_dict = dict(zip(feature_names, importances))

# Optional: Sort the dictionary by importance in descending order
feature_importance_sorted = dict(sorted(feature_importance_dict.items(), key=lambda item: item[1], reverse=True))

fig = go.Figure(data=[
        go.Bar(
            x=list(feature_importance_sorted.keys()),
            y=list(feature_importance_sorted.values()),
            marker=dict(color='skyblue'),
            text=[f"{v:.3f}" for v in feature_importance_sorted.values()],
            textposition='auto'
        )
    ])

# Update layout for better aesthetics
fig.update_layout(
    title='Feature Importances from Random Forest',
    xaxis_title='Features',
    yaxis_title="Importance Score",
    template='plotly_white',
    height=600
)

# Display the figure
fig.show()


#show decision tree
import matplotlib.pyplot as plt

plt.rcParams["figure.figsize"] = (80,40)
tree.plot_tree(rf,filled = True);
#plt.savefig('./images/tree_example.png')


from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from xgboost import XGBClassifier
# XGBoost
xgb = XGBClassifier(
    n_estimators=1000,
    learning_rate=0.3,
    max_depth=4,
    random_state=42,
    subsample=0.7,
    gamma=3,  # Increased from default
    reg_alpha=8,    # Added L1 regularization
    #reg_lambda=3,   # Added L2 regularization (can also be increased)
    eval_metric='mlogloss'
)

# Model pipeline
rf_pipe = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('xgboost', xgb)
])

# Preprocessing of training data, fit model 
rf_pipe.fit(X_train, y_train)

# Preprocessing of validation data, get predictions
rf_preds = rf_pipe.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, rf_preds)
print('Accuracy for XGBoost:', accuracy)

# Detailed classification report
print('Classification Report:\n', classification_report(y_test, rf_preds))


rf_preds = rf_pipe.predict(X_train)

# Evaluate the model
accuracy = accuracy_score(y_train, rf_preds)
print('Accuracy for Random Forest Model:', accuracy)

# Detailed classification report
print('Classification Report:\n', classification_report(y_train, rf_preds))




